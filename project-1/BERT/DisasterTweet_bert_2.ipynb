{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 17777,
     "databundleVersionId": 869809,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30648,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Downloaded all the necessary libararies\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch\n",
    "\n",
    "# Loading the training and testing data from CSV files\n",
    "train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "# Filling missing values in 'keyword' and 'location' columns with empty strings\n",
    "train_df['keyword'].fillna('', inplace=True)\n",
    "train_df['location'].fillna('', inplace=True)\n",
    "\n",
    "# Combining 'keyword', 'location', and 'text' columns into a single text column for processing\n",
    "train_df['combined_text'] = train_df['keyword'] + \" \" + train_df['location'] + \" \" + train_df['text']\n",
    "test_df['combined_text'] = test_df['keyword'] + \" \" + test_df['location'] + \" \" + test_df['text']\n",
    "\n",
    "# Preprocessing the combined text by converting to lowercase and removing special characters\n",
    "train_df['combined_text'] = train_df['combined_text'].str.lower().str.replace('[^a-z0-9\\s]', '', regex=True)\n",
    "test_df['combined_text'] = test_df['combined_text'].str.lower().str.replace('[^a-z0-9\\s]', '', regex=True)\n",
    "\n",
    "# Splitting the training data into a training set and a validation set for model evaluation\n",
    "train_text, val_text, train_labels, val_labels = train_test_split(\n",
    "    train_df['combined_text'], train_df['target'], test_size=0.1\n",
    ")\n",
    "\n",
    "# Initializing the tokenizer from the DistilBERT model, which is a lighter version of BERT\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenizing the training and validation text\n",
    "# This will convert text to a format that the model can understand\n",
    "train_encodings = tokenizer(train_text.tolist(), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_text.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Defining a custom dataset class to handle the PyTorch data loading\n",
    "class DisasterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# Creating dataset objects for training and validation\n",
    "train_dataset = DisasterDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = DisasterDataset(val_encodings, val_labels.tolist())\n",
    "\n",
    "# Initializing the DistilBERT model for sequence classification with two labels\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-02-01T23:22:47.699353Z",
     "iopub.execute_input": "2024-02-01T23:22:47.699921Z",
     "iopub.status.idle": "2024-02-01T23:22:48.874970Z",
     "shell.execute_reply.started": "2024-02-01T23:22:47.699886Z",
     "shell.execute_reply": "2024-02-01T23:22:48.873911Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": "/tmp/ipykernel_34/589905527.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  train_df['keyword'].fillna('', inplace=True)\n/tmp/ipykernel_34/589905527.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  train_df['location'].fillna('', inplace=True)\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Defining a function to compute evaluation metrics (F1 score and accuracy)\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "# Setting up training arguments, including directory for results, number of epochs, batch size, etc.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/kaggle/working/results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/kaggle/working/logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initializing the Trainer with the model, training arguments, datasets, and evaluation metrics\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Training the model on the dataset\n",
    "\n",
    "# Also downloaded transformers \n",
    "# pip install torch"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-01T23:22:48.876958Z",
     "iopub.execute_input": "2024-02-01T23:22:48.877325Z",
     "iopub.status.idle": "2024-02-01T23:22:48.965063Z",
     "shell.execute_reply.started": "2024-02-01T23:22:48.877292Z",
     "shell.execute_reply": "2024-02-01T23:22:48.963890Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-01T23:22:48.966484Z",
     "iopub.execute_input": "2024-02-01T23:22:48.967389Z",
     "iopub.status.idle": "2024-02-01T23:24:24.640610Z",
     "shell.execute_reply.started": "2024-02-01T23:22:48.967352Z",
     "shell.execute_reply": "2024-02-01T23:24:24.639504Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='645' max='645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [645/645 01:35, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.395000</td>\n      <td>0.480632</td>\n      <td>0.780840</td>\n      <td>0.765120</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.377200</td>\n      <td>0.430834</td>\n      <td>0.809711</td>\n      <td>0.778626</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.289700</td>\n      <td>0.473524</td>\n      <td>0.825459</td>\n      <td>0.801196</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
     "output_type": "stream"
    },
    {
     "execution_count": 6,
     "output_type": "execute_result",
     "data": {
      "text/plain": "TrainOutput(global_step=645, training_loss=0.39920383386833724, metrics={'train_runtime': 95.2421, 'train_samples_per_second': 215.797, 'train_steps_per_second': 6.772, 'total_flos': 324372556873404.0, 'train_loss': 0.39920383386833724, 'epoch': 3.0})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
